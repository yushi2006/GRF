# ğŸš€ Gated Recurrent Fusion (GRF): Efficient Multimodal Learning with Fewer Parameters

> ğŸ“„ Preprint: [arXiv:2507.02985](https://arxiv.org/pdf/2507.02985)  
> ğŸ§ª Submitted to: ACMMM 2025 Workshop UAVM
> ğŸ”¥ TL;DR: We propose **GRF**, a lightweight gated recurrent fusion module that outperforms MulT on CMU-MOSI unaligned setting using 3Ã— fewer parameters.

---

## ğŸ§  Overview

Multimodal models like MulT are powerful but suffer from computational overhead and sensitivity to alignment. We propose **Gated Recurrent Fusion (GRF)** â€” a sparse, parameter-efficient fusion module that captures cross-modal dynamics without relying on input alignment.

---

## ğŸ“¦ Features

- âœ… Gated Recurrent Fusion (GRF) for unaligned multimodal fusion  
- âœ… Modular design compatible with any modality order  
- âœ… Includes ablation studies and visualization tools  
- âœ… Reproducible experiments using config files and MLflow

---

## ğŸ“Š Results

| Model     | F1 Score | Parameters | Dataset    | Alignment |
|-----------|----------|------------|------------|-----------|
| MulT      | **81**   | ~8M        | CMU-MOSI   | Unaligned |
| GRF (Ours)| 79       | 4.5M       | CMU-MOSI   | Unaligned |

For full details, see the [arXiv paper](https://arxiv.org/pdf/2507.02985).

---

## ğŸ“ Project Structure
```
GRF
â”œâ”€â”€ src/ # GRF model definitions, data handlers, utils, and engine
â”œâ”€â”€ data/ # Dataloaders for CMU-MOSI
â”œâ”€â”€ train.py # Training script
â”œâ”€â”€ configs/ # YAML config files for training setups
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # You're here
```

## ğŸ”§ Installation

Clone the repo and install dependencies:

```bash
git clone https://github.com/yushi2006/GRF.git
cd GRF
pip install -r requirements.txt
```

## ğŸƒâ€â™‚ï¸ Quick Start

### Train the GRF Model

```bash
chmod +x run_experiments.sh

./run_experiments.sh
```

## ğŸ“š Citation
```
@misc{shihata2025gatedrecursivefusionstateful,
      title={Gated Recursive Fusion: A Stateful Approach to Scalable Multimodal Transformers}, 
      author={Yusuf Shihata},
      year={2025},
      eprint={2507.02985},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.02985}, 
}
```


