{
    "d_model": 256,
    "num_layers": 4,
    "d_ffn": 512,
    "num_heads": 4,
    "dropout": 0.1,
    "learning_rate": 1e-4,
    "weight_decay": 1e-3,
    "epochs": 100,
    "patience": 20,
    "warmup_epochs": 5,
    "batch_size": 32
}