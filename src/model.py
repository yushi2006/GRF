import torch
import torch.nn as nn

# Self-Attention block
class MultiSelfAttention(nn.Module):
    def __init__(self, embedding_size):
        super(MultiSelfAttention, self).__init__()

# Cross-Attention Block
class MultiCrossAttention(nn.Module):
    def __init__(self):
        super(MultiCrossAttention, self).__init__()

# FeedForward Block
class FeedForward(nn.Module):
    def __init__(self):
        super(FeedForward, self).__init__()

class ModalityAwareFusion(nn.Module):
    def __init__(self):
        super(ModalityAwareFusion, self).__init__()
